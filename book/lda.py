import nltk
import numpy
import re
#nltk.download('stopwords')
import pandas as pd
import numpy as np
import gensim
import string
from nltk.corpus import stopwords
from gensim.corpora import Dictionary
from gensim.models import LdaModel
from gensim import corpora
from gensim import models
import qalsadi.lemmatizer
stop = set(stopwords.words('arabic'))
stop_word_comp = {"،","غيرهما","قد ","تتكون","عليها","بأنها","آض","آمينَ","آه","آهاً","آي","أ","منها","ومنها","أب","أجل","أجمع","أخ","أخذ","أصبح","أضحى","أقبل","أقل","أكثر","ألا","أم","أما","أمامك","أمامكَ","أمسى","أمّا","أن","أنا","أنت","أنتم","أنتما","أنتن","أنتِ","أنشأ","أنّى","أو","أوشك","أولئك","أولئكم","أولاء","أولالك","أوّهْ","أي","أيا","أين","أينما","أيّ","أَنَّ","أََيُّ","أُفٍّ","إذ","إذا","إذاً","إذما","إذن","إلى","إليكم","إليكما","إليكنّ","إليكَ","إلَيْكَ","إلّا","إمّا","إن","إنّما","إي","إياك","إياكم","إياكما","إياكن","إيانا","إياه","إياها","إياهم","إياهما","إياهن","إياي","إيهٍ","إِنَّ","ا","ابتدأ","اثر","اجل","احد","اخرى","اخلولق","اذا","اربعة","ارتدّ","استحال","اطار","اعادة","اعلنت","اف","اكثر","اكد","الألاء","الألى","الا","الاخيرة","الان","الاول","الاولى","التى","التي","الثاني","الثانية","الذاتي","الذى","الذي","الذين","السابق","الف","اللائي","اللاتي","اللتان","اللتيا","اللتين","اللذان","اللذين","اللواتي","الماضي","المقبل","الوقت","الى","اليوم","اما","امام","امس","ان","انبرى","انقلب","انه","انها","او","اول","اي","ايار","ايام","ايضا","ب","بات","باسم","بان","بخٍ","برس","بسبب","بسّ","بشكل","بضع","بطآن","بعد","بعض","بك","بكم","بكما","بكن","بل","بلى","بما","بماذا","بمن","بن","بنا","به","بها","بي","بيد","بين","بَسْ","بَلْهَ","بِئْسَ","تانِ","تانِك","تبدّل","تجاه","تحوّل","تلقاء","تلك","تلكم","تلكما","تم","تينك","تَيْنِ","تِه","تِي","ثلاثة","ثم","ثمّ","ثمّة","ثُمَّ","جعل","جلل","جميع","جير","حار","حاشا","حاليا","حاي","حتى","حرى","حسب","حم","لأن","بهذه","لهذه","هى","حوالى","حول","حيث","حيثما","حين","حيَّ","حَبَّذَا","حَتَّى","حَذارِ","خلا","خلال","دون","دونك","ذا","ذات","ذاك","ذانك","ذانِ","ذلك","ذلكم","ذلكما","ذلكن","ذو","ذوا","ذواتا","ذواتي","ذيت","ذينك","ذَيْنِ","ذِه","ذِي","راح","رجع","رويدك","ريث","رُبَّ","زيارة","سبحان","سرعان","سنة","سنوات","سوف","سوى","سَاءَ","سَاءَمَا","شبه","شخصا","شرع","شَتَّانَ","صار","صباح","صفر","صهٍ","صهْ","ضد","ضمن","طاق","طالما","طفق","طَق","ظلّ","عاد","عام","عاما","عامة","عدا","عدة","عدد","عدم","عسى","عشر","عشرة","علق","على","عليك","عليه","عليها","علًّ","عن","عند","عندما","عوض","عين","عَدَسْ","عَمَّا","غدا","غير","ـ","ف","فان","فلان","فو","فى","في","فيم","فيما","فيه","فيها","قال","قام","قبل","قد","قطّ","قلما","قوة","كأنّما","كأين","كأيّ","كأيّن","كاد","كان","كانت","كذا","كذلك","كرب","كل","كلا","كلاهما","كلتا","كلم","كليكما","كليهما","كلّما","كلَّا","كم","كما","كي","كيت","كيف","كيفما","كَأَنَّ","كِخ","لئن","لا","لات","لاسيما","لدن","لدى","لعمر","لقاء","لك","لكم","لكما","لكن","لكنَّما","لكي","لكيلا","للامم","لم","لما","لمّا","لن","لنا","له","لها","لو","لوكالة","لولا","لوما","لي","لَسْتَ","لَسْتُ","لَسْتُم","لَسْتُمَا","لَسْتُنَّ","لَسْتِ","لَسْنَ","لَعَلَّ","لَكِنَّ","لَيْتَ","لَيْسَ","لَيْسَا","لَيْسَتَا","لَيْسَتْ","لَيْسُوا","لَِسْنَا","ما","ماانفك","مابرح","مادام","ماذا","مازال","مافتئ","مايو","متى","مثل","مذ","مساء","مع","معاذ","مقابل","مكانكم","مكانكما","مكانكنّ","مكانَك","مليار","مليون","مما","ممن","من","منذ","منها","مه","مهما","مَنْ","مِن","نحن","نحو","نعم","نفس","نفسه","نهاية","نَخْ","نِعِمّا","نِعْمَ","ها","هاؤم","هاكَ","هاهنا","هبّ","هذا","هذه","هكذا","هل","هلمَّ","هلّا","هم","هما","هن","هنا","هناك","هنالك","هو","هي","هيا","هيت","هيّا","هَؤلاء","هَاتانِ","هَاتَيْنِ","هَاتِه","هَاتِي","هَجْ","هَذا","هَذانِ","هَذَيْنِ","هَذِه","هَذِي","هَيْهَاتَ","و","و6","وا","واحد","و اضاف","و اضافت","و اكد","و ان","واهاً","و اوضح","وراءَك","و في","و قال","و قالت","و قد","وقف","و كان","و كانت","و لا","ولم","و من","مَن","و هو","و هي","ويكأنّ","وَيْ","وُ شْكَانََ","يكون","يمكن","يوم","ّأيّان"}
exclude = set(string.punctuation)

 

def key_word(content):

    def clean(content):
        stops =" ".join([w for w in content.split() if not w in stop and not w in stop_word_comp and len(w) >= 2])
        punc_free = ''.join(ch for ch in stops if ch not in exclude)
        words = re.sub("[^\w]", " ",  punc_free)
        return words

    #lemmatization 
    lemmer = qalsadi.lemmatizer.Lemmatizer()
    lemma = lemmer.lemmatize_text(clean(content))
    lemma = str(lemma).replace("جاف","جافا")
    lemma = str(lemma).replace("شرك","شركة")
    lemma = str(lemma).replace("طور","تطوير")
    lemma = str(lemma).replace("ريق","فريق")
    lemma = str(lemma).replace("صفح","صفحات")
    lemma = str(lemma).replace("ديناميك","ديناميكة")
    lemma = str(lemma).replace("كتاب","كتابة")
    lemma = str(lemma).replace("كتاب","كتابة")
    lemma = str(lemma).replace("صبح","أصبح")
    lemma = str(lemma).replace("بيان","بيانات")
    lemma = str(lemma).replace("تفاعل","تفاعلى")
    lemma = str(lemma).replace("افتراض","افتراضية")
    lemma = str(lemma).replace("ذاكر","ذاكرة")
    zzz = re.sub("ى", "ي", str(lemma))
    zzz = re.sub("1", "ات", zzz)
    zzz = re.sub("ة", "ه", zzz)
    zzz=re.sub("هه","ه",zzz)
    zzz = re.sub("2", "ن", zzz)
    zzz = re.sub("7", "ة", zzz)
    zzz = re.sub("فف", "ف", zzz)
    zzz = re.sub("طف", "ف", zzz)
    rrr=re.findall(r'(\w+)',zzz)
    docs =[rrr]
    #نجد انها تطبع الكلمات المتجاورة مع بعضها
    #
    bigram = gensim.models.phrases.Phrases(docs , min_count = 1, threshold = 5)
    trigram = gensim.models.phrases.Phrases(bigram[docs] ,min_count =1, threshold = 10)
    s = [trigram[bigram[docs[0]]]]
        #عمل حقيبة للكلمات
    dictionary = Dictionary(s)
    mycorpus = [dictionary.doc2bow(doc , allow_update = True) for doc in s ]
    #لطباعة الكلمة وكم مرة تكررت
    word_count = [[(dictionary[id] ,count) for id ,count in line] for line in mycorpus]
    #حفظ القاموس وحقيبة الكلمات
    dictionary.save("mydict1.dict")
    corpora.MmCorpus.serialize("bow_corpus.mm" , mycorpus)
    #عمل نموذج tfidf
    tfidf = models.TfidfModel(mycorpus , smartirs = 'ntc')
    #topic modeling
    #creating object for lda model
    lda = gensim.models.ldamodel.LdaModel
    #running and training lda model
    ldamodel = lda(corpus = tfidf[mycorpus] , num_topics = 1 , id2word = dictionary , passes = 50)
    topics = ldamodel.show_topics(num_words = 10, formatted = False)
    top =[]
    for i,s in topics:
        for b,n in s:
            top.append(b)
    
    top =[item.replace('_', ' ') for item in top]
        

    return top
